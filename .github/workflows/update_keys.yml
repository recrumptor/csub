name: Update VLESS Keys

on:
  schedule:
    - cron: '0 */6 * * *'
  workflow_dispatch:

permissions:
  contents: write

jobs:
  update:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Process Keys
        run: |
          python3 - << 'EOF'
          import urllib.parse as urlparse
          import requests

          unique_configs = {}
          total_links = 0
          duplicate_count = 0

          # Читаем ссылки из site.txt
          try:
              with open('site.txt', 'r') as f:
                  urls = [line.strip() for line in f if line.strip()]
          except FileNotFoundError:
              print("site.txt не найден")
              exit(1)

          def normalize_link(line):
              # 1. Отсекаем имя (#)
              base = line.split('#')[0].strip()
              if '?' not in base:
                  return base.lower().strip()

              # 2. Разделяем адрес и параметры
              base_part, params_str = base.split('?', 1)
              base_part = base_part.lower().strip().rstrip('/') # Убираем / в конце адреса
              base_part = base_part.rsplit(':', 1)[0] + ':' + base_part.rsplit(':', 1)[1].rstrip('/')

              # 3. Игнорируем мусорные параметры
              ignore_keys = {
                  "name", "remark", "peer", "alpn", "flow", "tls",
                  "allowinsecure", "packetencoding", "aid", "ps",
                  "group", "tag", "v", "sid", "encryption", "fp",
                  "headertype", "type"
              }

              params = {}
              for p in params_str.split('&'):
                  if '=' in p:
                      k, v = p.split('=', 1)
                      k = k.lower().strip()
                      if k in ignore_keys:
                          continue
                      v = urlparse.unquote(v).strip()
                      if v == "":
                          continue
                      params[k] = v
                  elif p:
                      continue

              # 4. Сортировка параметров
              normalized_params = "&".join(f"{k}={params[k]}" for k in sorted(params))
              return f"{base_part}?{normalized_params}"

          for url in urls:
              try:
                  print(f"Загрузка: {url}")
                  response = requests.get(url, timeout=15)

                  for line in response.text.splitlines():
                      line = line.strip()
                      if not line or '://' not in line:
                          continue

                      total_links += 1
                      norm = normalize_link(line)

                      if norm not in unique_configs:
                          unique_configs[norm] = line
                      else:
                          duplicate_count += 1
                          print(f"--- Дубликат найден: {norm} ---")

              except Exception as e:
                  print(f"Ошибка при работе с {url}: {e}")

          # Сохраняем результат
          with open('vless_list.txt', 'w', encoding='utf-8') as f:
              for original in unique_configs.values():
                  f.write(original + '\n')

          # Итоговая статистика
          print("\n===== СТАТИСТИКА =====")
          print(f"Всего ссылок получено: {total_links}")
          print(f"Уникальных после нормализации: {len(unique_configs)}")
          print(f"Удалено как дубликаты: {duplicate_count}")
          print("=======================\n")
          EOF

      - name: Commit and Push
        run: |
          git config --global user.name "github-actions[bot]"
          git config --global user.email "github-actions[bot]@users.noreply.github.com"
          git add vless_list.txt
          git commit -m "Update unique keys" || exit 0
          git push
